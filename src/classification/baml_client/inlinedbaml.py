###############################################################################
#
#  Welcome to Baml! To use this generated code, please run the following:
#
#  $ pip install baml-py
#
###############################################################################

# This file was generated by BAML: please do not edit it. Instead, edit the
# BAML files and re-generate this code.
#
# ruff: noqa: E501,F401,F821
# flake8: noqa: E501,F401,F821
# pylint: disable=unused-import,line-too-long
# fmt: off

file_map = {
    
    "clients.baml": "// Learn more about clients at https://docs.boundaryml.com/docs/snippets/clients/overview\n\nclient<llm> GTP4oMini {\n  provider openai\n  options {\n    model \"gpt-4o-mini\"\n    api_key env.OPENAI_API_KEY\n    temperature 0.1\n  }\n}\n\nclient<llm> OpenRouterGPT4oMini {\n  provider \"openai-generic\"\n  options {\n    base_url \"https://openrouter.ai/api/v1\"\n    api_key env.OPENROUTER_API_KEY\n    model \"openai/gpt-4o-mini\"\n    temperature 0.1\n    headers {\n      \"HTTP-Referer\" \"https://thedataquarry.com\" // Optional\n      \"X-Title\" \"thedataquarry\" // Optional\n    }\n  }\n}\n\n// Custom LLM inference server by YourTechBud (wrapper for Ollama)\nclient<llm> OllamaGemma3_27b {\n  provider openai-generic\n  options {\n    base_url \"https://inferix.yourtechbud.studio/inferix/v1/llm\"\n    api_key env.INFERIX_API_KEY\n    model \"gemma3:27b\"\n    max_tokens 500\n    temperature 1.0\n    top_k 64\n    top_p 0.95\n    min_p 0.0\n  }\n}\n\nclient<llm> OllamaGemma3_12b {\n  provider openai-generic\n  options {\n    base_url \"https://inferix.yourtechbud.studio/inferix/v1/llm\"\n    api_key env.INFERIX_API_KEY\n    model \"gemma3:12b\"\n    max_tokens 500\n    temperature 0.1\n  }\n}\n\n// https://docs.boundaryml.com/docs/snippets/clients/retry\nretry_policy Constant {\n  max_retries 3\n  // Strategy is optional\n  strategy {\n    type constant_delay\n    delay_ms 200\n  }\n}\n\nretry_policy Exponential {\n  max_retries 2\n  // Strategy is optional\n  strategy {\n    type exponential_backoff\n    delay_ms 300\n    multiplier 1.5\n    max_delay_ms 10000\n  }\n}",
    "genderize.baml": "// Acceptable outputs from the LLM\nenum Gender {\n  Male\n  Female\n  Unknown @description(\"When you are not sure of the final answer\")\n}\n\n// Conventional prompting\nfunction ClassifyGender(info: string) -> Gender {\n  client OllamaGemma3_27b\n  prompt #\"\n    Based on your knowledge of historical scholars and scientists, determine the likely gender of this person.\n    Scholars who are termed \"laureates\" won the Nobel Prize in a category.\n\n    ONLY respond with one gender.\n\n    {{ ctx.output_format }}\n\n    {{ _.role(\"user\") }}\n    {{ info }}\n  \"#\n}\n\n// Chain-of-Draft (CoD) prompting\nfunction ClassifyGenderCoD(info: string) -> Gender {\n  client OllamaGemma3_27b\n  prompt #\"\n    Based on your knowledge of historical scholars and scientists, determine the likely gender of this person.\n    Scholars who are termed \"laureates\" won the Nobel Prize in a category.\n\n    ONLY respond with one gender.\n\n    {{ ctx.output_format }}\n\n    Think step by step before answering. Only keep a minimum draft for each thinking step, with 10 words at most. Make sure you try to recall the answer from your internal memory.\n    Return the final JSON object at the end of the thinking process.\n\n    {{ _.role(\"user\") }}\n    {{ info }}\n  \"#\n}\n\n// Conventional prompting tests\ntest ClassifyAaronKlug {\n  functions [ClassifyGender]\n  args {\n    info \"name: Aaron Klug\\ninfo: 1982 Chemistry Nobel Prize\"\n  }\n}\n\n\ntest ClassifyAbbaLerner {\n  functions [ClassifyGender]\n  args {\n    info \"name: Abba Lerner\\ninfo: 1974 Physics Nobel Prize\"\n  }\n}\n\ntest ClassifyAJFMBrochantDeVilliers {\n  functions [ClassifyGender]\n  args {\n    info \"name: AJFM Brochant de Villiers\\ninfo: scholar\"\n  }\n}\n\ntest ClassifyElaineTuomanen {\n  functions [ClassifyGender]\n  args {\n    info \"name: Elaine Tuomanen\\ninfo: scholar\"\n  }\n}\n\ntest ClassifyAbdusSalam {\n  functions [ClassifyGender]\n  args {\n    info \"name: Abdus Salam\\ninfo: 1979 Physics Nobel Prize\"\n  }\n}\n\ntest ClassifyAndreaGhez {\n  functions [ClassifyGender]\n  args {\n    info \"name: Andrea Ghez\\ninfo: 2020 Physics Nobel Prize\"\n  }\n}\n\n// Test with Chain-of-Draft (CoD) prompting\ntest ClassifyAimeCotton {\n  functions [ClassifyGenderCoD]\n  args {\n    info \"name: Aime Cotton\\ninfo: scholar\"\n  }\n}\n\ntest ClassifyLeonorMichaelis {\n  functions [ClassifyGenderCoD]\n  args {\n    info \"name: Leonor Michaelis\\ninfo: scholar\"\n  }\n}",
    "generators.baml": "// This helps use auto generate libraries you can use in the language of\n// your choice. You can have multiple generators if you use multiple languages.\n// Just ensure that the output_dir is different for each generator.\ngenerator target {\n    // Valid values: \"python/pydantic\", \"typescript\", \"ruby/sorbet\", \"rest/openapi\"\n    output_type \"python/pydantic\"\n\n    // Where the generated code will be saved (relative to baml_src/)\n    output_dir \"../\"\n\n    // The version of the BAML package you have installed (e.g. same version as your baml-py or @boundaryml/baml).\n    // The BAML VSCode extension version should also match this version.\n    version \"0.80.0\"\n\n    // Valid values: \"sync\", \"async\"\n    // This controls what `b.FunctionName()` will be (sync or async).\n    default_client_mode sync\n}\n",
}

def get_baml_files():
    return file_map